{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8443ab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.metrics import confusion_matrix, make_scorer\n",
    "from sklearn.ensemble import GradientBoostingClassifier, IsolationForest ,HistGradientBoostingClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import lightgbm as lgb\n",
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2343ead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"aps_failure_training_set.csv\", skiprows=20,na_values='na')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8adfd66",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Objective and Problem Statement\n",
    "\n",
    "This dataset contains operational and sensor data collected from **Scania heavy-duty trucks**, focusing on the **Air Pressure System (APS)**, which supplies compressed air used in functions such as braking and gear changes.\n",
    "\n",
    "### Task:\n",
    "Build a **binary classification model** to predict:\n",
    "- `pos`: Failure in a **specific component** of the APS system,\n",
    "- `neg`: Failures **unrelated** to the APS.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“¦ Dataset Characteristics:\n",
    "- 60,000 samples in the training set:\n",
    "  - 1,000 `pos` (APS-related failures)\n",
    "  - 59,000 `neg` (non-APS failures)\n",
    "- 171 numerical features (including histograms and counters)\n",
    "- Contains missing values (`NaN`)\n",
    "- Highly imbalanced: ~1.7% positive class\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ Business Metric: Total Cost of Misclassification\n",
    "\n",
    "The goal is to **minimize the cost of misclassification**, not just improve accuracy or F1-score.\n",
    "\n",
    "| Predicted class | True class | Cost         |\n",
    "|-----------------|------------|--------------|\n",
    "| `pos`           | `neg`      | 10 (false alarm â€” unnecessary mechanic check) |\n",
    "| `neg`           | `pos`      | 500 (missed failure â€” potential truck breakdown) |\n",
    "\n",
    "### Cost formula:\n",
    "Total_cost = 10 Ã— FP + 500 Ã— FN\n",
    "\n",
    "- **FP (False Positives):** Truck is wrongly flagged â†’ unnecessary service\n",
    "- **FN (False Negatives):** Truck fault is missed â†’ high-impact breakdown\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Modeling Goal\n",
    "\n",
    "> Build a robust predictive model that **minimizes the `Total_cost`**,  \n",
    "> even if it means tolerating more false positives in exchange for fewer false negatives.\n",
    "\n",
    "This reflects the **real-world business trade-off**, where missing a true APS failure is far more costly than investigating a false one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce8d810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 171)\n",
      "neg    59000\n",
      "pos     1000\n",
      "Name: class, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>aa_000</th>\n",
       "      <th>ab_000</th>\n",
       "      <th>ac_000</th>\n",
       "      <th>ad_000</th>\n",
       "      <th>ae_000</th>\n",
       "      <th>af_000</th>\n",
       "      <th>ag_000</th>\n",
       "      <th>ag_001</th>\n",
       "      <th>ag_002</th>\n",
       "      <th>...</th>\n",
       "      <th>ee_002</th>\n",
       "      <th>ee_003</th>\n",
       "      <th>ee_004</th>\n",
       "      <th>ee_005</th>\n",
       "      <th>ee_006</th>\n",
       "      <th>ee_007</th>\n",
       "      <th>ee_008</th>\n",
       "      <th>ee_009</th>\n",
       "      <th>ef_000</th>\n",
       "      <th>eg_000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>76698</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.130706e+09</td>\n",
       "      <td>280.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1240520.0</td>\n",
       "      <td>493384.0</td>\n",
       "      <td>721044.0</td>\n",
       "      <td>469792.0</td>\n",
       "      <td>339156.0</td>\n",
       "      <td>157956.0</td>\n",
       "      <td>73224.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>33058</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>421400.0</td>\n",
       "      <td>178064.0</td>\n",
       "      <td>293306.0</td>\n",
       "      <td>245416.0</td>\n",
       "      <td>133654.0</td>\n",
       "      <td>81140.0</td>\n",
       "      <td>97576.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neg</td>\n",
       "      <td>41040</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.280000e+02</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>277378.0</td>\n",
       "      <td>159812.0</td>\n",
       "      <td>423992.0</td>\n",
       "      <td>409564.0</td>\n",
       "      <td>320746.0</td>\n",
       "      <td>158022.0</td>\n",
       "      <td>95128.0</td>\n",
       "      <td>514.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.000000e+01</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>240.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>60874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.368000e+03</td>\n",
       "      <td>458.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>622012.0</td>\n",
       "      <td>229790.0</td>\n",
       "      <td>405298.0</td>\n",
       "      <td>347188.0</td>\n",
       "      <td>286954.0</td>\n",
       "      <td>311560.0</td>\n",
       "      <td>433954.0</td>\n",
       "      <td>1218.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 171 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  class  aa_000  ab_000        ac_000  ad_000  ae_000  af_000  ag_000  ag_001  \\\n",
       "0   neg   76698     NaN  2.130706e+09   280.0     0.0     0.0     0.0     0.0   \n",
       "1   neg   33058     NaN  0.000000e+00     NaN     0.0     0.0     0.0     0.0   \n",
       "2   neg   41040     NaN  2.280000e+02   100.0     0.0     0.0     0.0     0.0   \n",
       "3   neg      12     0.0  7.000000e+01    66.0     0.0    10.0     0.0     0.0   \n",
       "4   neg   60874     NaN  1.368000e+03   458.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   ag_002  ...     ee_002    ee_003    ee_004    ee_005    ee_006    ee_007  \\\n",
       "0     0.0  ...  1240520.0  493384.0  721044.0  469792.0  339156.0  157956.0   \n",
       "1     0.0  ...   421400.0  178064.0  293306.0  245416.0  133654.0   81140.0   \n",
       "2     0.0  ...   277378.0  159812.0  423992.0  409564.0  320746.0  158022.0   \n",
       "3     0.0  ...      240.0      46.0      58.0      44.0      10.0       0.0   \n",
       "4     0.0  ...   622012.0  229790.0  405298.0  347188.0  286954.0  311560.0   \n",
       "\n",
       "     ee_008  ee_009  ef_000  eg_000  \n",
       "0   73224.0     0.0     0.0     0.0  \n",
       "1   97576.0  1500.0     0.0     0.0  \n",
       "2   95128.0   514.0     0.0     0.0  \n",
       "3       0.0     0.0     4.0    32.0  \n",
       "4  433954.0  1218.0     0.0     0.0  \n",
       "\n",
       "[5 rows x 171 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a490cde",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Target Variable Overview (`class`)\n",
    "\n",
    "- Total samples: **60,000**\n",
    "- Class distribution:\n",
    "  - `neg` (negative class â€“ faults **not related** to APS): **59,000**\n",
    "  - `pos` (positive class â€“ faults **in the APS component**): **1,000**\n",
    "\n",
    "> **Note:** The dataset is highly imbalanced (~1.7% positive class). This imbalance should be addressed using:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bab478e",
   "metadata": {},
   "source": [
    "## ðŸ” Data Quality Assessment: Duplicate and Missing Values\n",
    "\n",
    "Ensuring data integrity is a foundational step in any machine learning pipeline. Here we perform two essential checks:\n",
    "\n",
    "- **Duplicate Entries**  \n",
    "\n",
    "- **Missing Values (`NaN`)**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62210c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850015\n",
      "br_000    82.106667\n",
      "bq_000    81.203333\n",
      "bp_000    79.566667\n",
      "bo_000    77.221667\n",
      "ab_000    77.215000\n",
      "            ...    \n",
      "cj_000     0.563333\n",
      "ci_000     0.563333\n",
      "bt_000     0.278333\n",
      "aa_000     0.000000\n",
      "class      0.000000\n",
      "Length: 171, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum().sum())  # Total number of missing values\n",
    "\n",
    "na_percent = df.isna().mean() * 100\n",
    "print(na_percent.sort_values(ascending=False))  # % missing per column, descending "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a782ab7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "dup_mask = df.duplicated()\n",
    "print(dup_mask.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a362c4",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Data Quality Assessment: Duplicate and Missing Values\n",
    "\n",
    "- **Missing values detected**: `850,015`  \n",
    "- **Duplicate rows found**: `0`\n",
    "\n",
    "---\n",
    "\n",
    "> âœ… **Conclusion**  \n",
    "> The dataset contains no duplicate entries. However, a significant number of missing values must be addressed before further analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85125c7a",
   "metadata": {},
   "source": [
    "## ðŸ” Feature/Target Definition and Train-Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce86cc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('class', axis=1)\n",
    "y = df['class'].map({'pos': 1, 'neg': 0})         # Convert class labels to binary\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0b0e52",
   "metadata": {},
   "source": [
    "## ðŸ” Missing Values in Training Set\n",
    "\n",
    "Analyzing the proportion of missing values across features in the training dataset.\n",
    "\n",
    "- **Some features have >70% missing values**\n",
    "- **Dropping features based on threshold alone is not appropriate**\n",
    "\n",
    "---\n",
    "\n",
    "> ðŸ“Œ **Conclusion**  \n",
    "> Several features contain high missing rates, but cannot be discarded blindly due to potential importance (e.g., critical sensors). I will test five different imputation strategies to determine the most effective approach for this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f96e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Imputation  Avg_Total_Cost\n",
      "2     constant_0    46840.000000\n",
      "0         median    47210.000000\n",
      "3            knn    47526.666667\n",
      "4    median+mask    47533.333333\n",
      "1  most_frequent    47870.000000\n"
     ]
    }
   ],
   "source": [
    "def total_cost(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return -(10 * fp + 500 * fn)\n",
    "\n",
    "cost_scorer = make_scorer(total_cost, greater_is_better=True)\n",
    "\n",
    "# ðŸ“Œ Transformator do maski brakÃ³w danych\n",
    "class MissingIndicatorAdder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col in X.columns:\n",
    "            if X[col].isna().any():\n",
    "                X[f'{col}_missing'] = X[col].isna().astype(int)\n",
    "        return X\n",
    "\n",
    "# ðŸ“Œ Strategie imputacji\n",
    "imputers = {\n",
    "    'median': SimpleImputer(strategy='median'),\n",
    "    'most_frequent': SimpleImputer(strategy='most_frequent'),\n",
    "    'constant_0': SimpleImputer(strategy='constant', fill_value=0),\n",
    "    'knn': KNNImputer(n_neighbors=5),  # uwaga: wolne!\n",
    "    'median+mask': Pipeline([\n",
    "        ('mask', MissingIndicatorAdder()),\n",
    "        ('impute', SimpleImputer(strategy='median'))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# ðŸ“Œ GÅ‚Ã³wna funkcja porÃ³wnujÄ…ca strategie imputacji\n",
    "def compare_imputation_strategies(X_train, y_train):\n",
    "    results = []\n",
    "\n",
    "    # âœ… 3-krotna stratyfikowana walidacja krzyÅ¼owa\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "    for name, imputer in imputers.items():\n",
    "        pipe = Pipeline([\n",
    "            ('imputer', imputer),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', GradientBoostingClassifier(random_state=42))\n",
    "        ])\n",
    "\n",
    "        # â± Trening + walidacja\n",
    "        score = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=cost_scorer, n_jobs=-1).mean()\n",
    "        results.append((name, -score))  # minus usuwamy â€” chcemy prawdziwy koszt\n",
    "\n",
    "    return pd.DataFrame(results, columns=[\"Imputation\", \"Avg_Total_Cost\"]).sort_values(by=\"Avg_Total_Cost\")\n",
    "\n",
    "results_df = compare_imputation_strategies(X_train, y_train)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c20e58",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Imputation Strategy Evaluation (Initial)\n",
    "\n",
    "Compared five imputation methods using average total cost on a holdout set.\n",
    "\n",
    "- **Best methods**: `constant_0` and `median`\n",
    "\n",
    "---\n",
    "\n",
    "> **Conclusion**  \n",
    "> Selected the two best-performing strategies (`constant_0` and `median`) for further comparison using 10-fold cross-validation to determine the most reliable imputation approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f0158c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Imputation  Avg_Total_Cost\n",
      "0         median         12851.0\n",
      "1  most_frequent         13052.0\n"
     ]
    }
   ],
   "source": [
    "def total_cost(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return -(10 * fp + 500 * fn)\n",
    "\n",
    "cost_scorer = make_scorer(total_cost, greater_is_better=True)\n",
    "\n",
    "# ðŸ“Œ Ð¢Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼ÐµÑ€ Ð´Ð»Ñ Ð¼Ð°ÑÐºÐ¸ Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ¾Ð²\n",
    "class MissingIndicatorAdder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col in X.columns:\n",
    "            if X[col].isna().any():\n",
    "                X[f'{col}_missing'] = X[col].isna().astype(int)\n",
    "        return X\n",
    "\n",
    "\n",
    "imputers = {\n",
    "    'median': SimpleImputer(strategy='median'),\n",
    "    'most_frequent': SimpleImputer(strategy='most_frequent'),\n",
    "    #'constant_0': SimpleImputer(strategy='constant', fill_value=0),\n",
    "    #'knn': KNNImputer(n_neighbors=5),  \n",
    "    #'median+mask': Pipeline([\n",
    "    #    ('mask', MissingIndicatorAdder()),\n",
    "    #    ('impute', SimpleImputer(strategy='median'))\n",
    "    #])\n",
    "}\n",
    "\n",
    "# ðŸ“Œ GÅ‚Ã³wna funkcja porÃ³wnujÄ…ca strategie imputacji\n",
    "def compare_imputation_strategies(X_train, y_train):\n",
    "    results = []\n",
    "\n",
    "    # âœ… 10-krotna stratyfikowana walidacja krzyÅ¼owa\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "    for name, imputer in imputers.items():\n",
    "        pipe = Pipeline([\n",
    "            ('imputer', imputer),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', GradientBoostingClassifier(random_state=42))\n",
    "        ])\n",
    "\n",
    "        # â± Trening + walidacja\n",
    "        score = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=cost_scorer, n_jobs=-1).mean()\n",
    "        results.append((name, -score))  # minus usuwamy â€” chcemy prawdziwy koszt\n",
    "\n",
    "    return pd.DataFrame(results, columns=[\"Imputation\", \"Avg_Total_Cost\"]).sort_values(by=\"Avg_Total_Cost\")\n",
    "\n",
    "results_df = compare_imputation_strategies(X_train, y_train)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82e96da",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Imputation Strategy Comparison (10-Fold CV)\n",
    "\n",
    "Final evaluation of top imputation techniques using cross-validation.\n",
    "\n",
    "- `median`: **12851.0**\n",
    "- `most_frequent`: 13052.0\n",
    "\n",
    "---\n",
    "\n",
    ">**Conclusion**  \n",
    "> Based on 10-fold cross-validation, median imputation achieved the lowest average total cost. This method will be used going forward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d40fbe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "X_train_imp = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_imp = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0417b799",
   "metadata": {},
   "source": [
    "### ðŸ” Isolation Forest (Outlier flag)\n",
    "\n",
    "Applying the **Isolation Forest** technique to detect outliers in the dataset. I chose this method as an initial experiment because it seemed the most promising for identifying anomalies that could affect model performance. A binary flag `is_outlier` is added to indicate whether a given observation is considered an outlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41079bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\igork\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\igork\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg | VAL cost = 7410 @ thr=0.67\n",
      "rf    | VAL cost = 6440 @ thr=0.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\igork\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "et    | VAL cost = 6150 @ thr=0.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\igork\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gb    | VAL cost = 6960 @ thr=0.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\igork\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hgb   | VAL cost = 6430 @ thr=0.12\n",
      "xgb   | VAL cost = 10950 @ thr=0.62\n",
      "\n",
      "Top-2 Ð¿Ð¾ Ð²Ð°Ð»Ð¸Ð´aÑ†Ð¸Ð¸:\n",
      "   et : Cost=6150, thr=0.16\n",
      "  hgb : Cost=6430, thr=0.12\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. Data\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "train_df = pd.read_csv(\"aps_failure_training_set.csv\", skiprows=20, na_values='na')\n",
    "test_df  = pd.read_csv(\"aps_failure_test_set.csv\", skiprows=20, na_values='na')\n",
    "\n",
    "y_train = train_df.pop(\"class\").map({'pos': 1, 'neg': 0})\n",
    "y_test  = test_df.pop(\"class\").map({'pos': 1, 'neg': 0})\n",
    "X_train_raw = train_df\n",
    "X_test_raw  = test_df\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. Na imputation\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train_imp = pd.DataFrame(imputer.fit_transform(X_train_raw), columns=X_train_raw.columns)\n",
    "X_test_imp  = pd.DataFrame(imputer.transform(X_test_raw), columns=X_test_raw.columns)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. Castom cost function\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def total_cost(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return -(10 * fp + 500 * fn)\n",
    "\n",
    "cost_scorer = make_scorer(total_cost, greater_is_better=True)\n",
    "w_train = np.where(y_train == 1, 500, 10)\n",
    "w_val   = np.where(y_test == 1, 500, 10)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4. Castom transformer for outlier detection\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class OutlierFlagAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, contamination=0.01, random_state=42):\n",
    "        self.contamination = contamination\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.iso = IsolationForest(contamination=self.contamination, random_state=self.random_state)\n",
    "        self.iso.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        X_['is_outlier'] = (self.iso.predict(X) == -1).astype(int)\n",
    "        return X_\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5. Preprocessor\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def make_preprocessor(feature_names):\n",
    "    return ColumnTransformer([\n",
    "        ('scale', StandardScaler(), feature_names + ['is_outlier']),  \n",
    "    ])\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 6. models + hyperparameters\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "search_space = {\n",
    "    'logreg': (\n",
    "        LogisticRegression(max_iter=2000),\n",
    "        {'model__C': np.logspace(-3, 2, 12),\n",
    "         'model__solver': ['lbfgs', 'saga']}\n",
    "    ),\n",
    "    'rf': (\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        {'model__n_estimators': [400, 600],\n",
    "         'model__max_depth':   [None, 20]}\n",
    "    ),\n",
    "    'et': (\n",
    "        ExtraTreesClassifier(random_state=42),\n",
    "        {'model__n_estimators': [400, 600],\n",
    "         'model__max_depth':   [None, 20]}\n",
    "    ),\n",
    "    'gb': (\n",
    "        GradientBoostingClassifier(random_state=42),\n",
    "        {'model__n_estimators': [300, 500],\n",
    "         'model__learning_rate': [0.03, 0.07],\n",
    "         'model__max_depth': [2, 3]}\n",
    "    ),\n",
    "    'hgb': (\n",
    "        HistGradientBoostingClassifier(random_state=42),\n",
    "        {'model__max_iter': [300, 500],\n",
    "         'model__learning_rate': [0.03, 0.07],\n",
    "         'model__max_depth': [None, 6]}\n",
    "    ),\n",
    "    'xgb': (\n",
    "        xgb.XGBClassifier(\n",
    "            objective='binary:logistic',\n",
    "            eval_metric='logloss',\n",
    "            random_state=42,\n",
    "            tree_method='hist'),\n",
    "        {'model__n_estimators': [400, 600],\n",
    "         'model__max_depth': [3, 5],\n",
    "         'model__learning_rate': [0.05],\n",
    "         'model__subsample': [0.8, 1.0],\n",
    "         'model__colsample_bytree': [0.8, 1.0],\n",
    "         'model__scale_pos_weight': [50, 59]}\n",
    "    )\n",
    "}\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 7. Seaarch and validation\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "best_on_val = []\n",
    "feature_names = X_train_imp.columns.tolist()\n",
    "\n",
    "for name, (est, grid) in search_space.items():\n",
    "    pipe = Pipeline([\n",
    "        ('outliers', OutlierFlagAdder(contamination=0.01, random_state=42)),\n",
    "        ('prep', make_preprocessor(feature_names)),\n",
    "        ('model', est)\n",
    "    ])\n",
    "\n",
    "    rs = RandomizedSearchCV(\n",
    "        pipe, grid, n_iter=10,\n",
    "        scoring=cost_scorer, cv=cv,\n",
    "        n_jobs=-1, random_state=42, verbose=0\n",
    "    )\n",
    "\n",
    "    rs.fit(X_train_imp, y_train, model__sample_weight=w_train)\n",
    "    best = rs.best_estimator_\n",
    "\n",
    "    # â”€â”€â”€ Prediction on validation dataset â”€â”€â”€\n",
    "    probs = best.predict_proba(X_test_imp)[:, 1]\n",
    "    ts = np.linspace(0, 1, 101)\n",
    "    best_thr, best_cost = 0.5, np.inf\n",
    "    for t in ts:\n",
    "        preds = (probs >= t).astype(int)\n",
    "        cost = 10 * ((preds == 1) & (y_test == 0)).sum() + 500 * ((preds == 0) & (y_test == 1)).sum()\n",
    "        if cost < best_cost:\n",
    "            best_cost, best_thr = cost, t\n",
    "\n",
    "    best_on_val.append((name, best, best_thr, best_cost))\n",
    "    print(f\"{name:5s} | VAL cost = {best_cost:.0f} @ thr={best_thr:.2f}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 8. Top 2 by validation\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "best_on_val.sort(key=lambda x: x[3])\n",
    "print(\"\\nTop 2 by validation:\")\n",
    "for name, _, thr, cost in best_on_val[:2]:\n",
    "    print(f\"{name:>5s} : Cost={cost:.0f}, thr={thr:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30f1a38",
   "metadata": {},
   "source": [
    "### ðŸ“Œ Conclusion\n",
    "\n",
    "Among the six evaluated models, **Extra Trees (ET)** and **HistGradientBoosting (HGB)** achieved the lowest validation costs, with `ET` showing the best performance at a cost of 6150 and an optimal threshold of 0.16. The `HGB` model followed closely with a cost of 6430 at threshold 0.12. Despite some warnings (e.g., parameter grid size limitations and convergence issues in logistic regression), the evaluation process successfully identified the top-performing models based on a cost-sensitive validation strategy. These results indicate that tree-based ensemble methods are the most effective for this task under the defined cost function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430a48bb",
   "metadata": {},
   "source": [
    "### ðŸ” Outlier handling strategies â€“ feature engineering and comparison\n",
    "\n",
    "In this section, I explored multiple strategies for incorporating outlier information into the model pipeline. Using both **Isolation Forest** and **Local Outlier Factor (LOF)**, I engineered new features representing outlier flags and scores.\n",
    "\n",
    "After generating these features, I constructed several variants of the training data to test different approaches to handling outliers:\n",
    "\n",
    "- **`baseline`**: no outlier information used.\n",
    "- **`with_flag`**: binary flag from Isolation Forest included as a feature.\n",
    "- **`with_score`**: raw outlier score included as a feature instead of a flag.\n",
    "- **`without_outliers`**: all outliers removed based on Isolation Forest.\n",
    "- **`lof_flag`**: binary LOF-based outlier flag used instead of Isolation Forest.\n",
    "\n",
    "For each variant, I trained a `HistGradientBoostingClassifier`, tuned the classification threshold using a domain-specific cost function (heavily penalizing false negatives), and evaluated performance on a validation set. The results were summarized in a comparison table, enabling a clear assessment of which outlier handling strategy led to the lowest total cost.\n",
    "\n",
    "This approach provides an interpretable and systematic way to quantify the value of outlier-related features in the modeling pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad798dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¶ Strategia: baseline\n",
      "â–¶ Strategia: with_flag\n",
      "â–¶ Strategia: with_score\n",
      "â–¶ Strategia: without_outliers\n",
      "â–¶ Strategia: lof_flag\n",
      "\n",
      "ðŸ“Š PorÃ³wnanie strategii obsÅ‚ugi obserwacji odstajÄ…cych:\n",
      "             Metoda  Koszt_Calkowity  Najlepszy_Prog\n",
      "0        with_score             6520           0.010\n",
      "1          baseline             6700           0.005\n",
      "2         with_flag             6700           0.005\n",
      "3          lof_flag             7460           0.010\n",
      "4  without_outliers             8220           0.010\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 0. Dane wejÅ›ciowe (train / val juÅ¼ przygotowane)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "X_tr = X_train_imp.copy().reset_index(drop=True)\n",
    "y_tr = y_train.copy().reset_index(drop=True)\n",
    "\n",
    "X_val = X_test_imp.copy().reset_index(drop=True)\n",
    "y_val = y_test.copy().reset_index(drop=True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. IsolationForest + LOF â†’ nowe cechy w X_tr\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "iso = IsolationForest(contamination=0.01, random_state=42)\n",
    "iso.fit(X_tr)\n",
    "X_tr['is_outlier'] = (iso.predict(X_tr) == -1).astype(int)\n",
    "X_tr['outlier_score'] = -iso.decision_function(X_tr)\n",
    "\n",
    "lof = LocalOutlierFactor(n_neighbors=20)\n",
    "X_tr['is_lof'] = (lof.fit_predict(X_tr.drop(columns=['is_outlier', 'outlier_score'])) == -1).astype(int)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. Przygotowanie sÅ‚ownika wariantÃ³w\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "mask_no_out = X_tr['is_outlier'] == 0\n",
    "\n",
    "variants = {\n",
    "    'baseline': (\n",
    "        X_tr.drop(columns=['is_outlier', 'outlier_score', 'is_lof']),\n",
    "        y_tr\n",
    "    ),\n",
    "    'with_flag': (\n",
    "        X_tr.drop(columns=['outlier_score', 'is_lof']),\n",
    "        y_tr\n",
    "    ),\n",
    "    'with_score': (\n",
    "        X_tr.drop(columns=['is_outlier', 'is_lof']),\n",
    "        y_tr\n",
    "    ),\n",
    "    'without_outliers': (\n",
    "        X_tr.loc[mask_no_out].drop(columns=['is_outlier', 'outlier_score', 'is_lof']),\n",
    "        y_tr.loc[mask_no_out]\n",
    "    ),\n",
    "    'lof_flag': (\n",
    "        X_tr.drop(columns=['is_outlier', 'outlier_score']),\n",
    "        y_tr\n",
    "    )\n",
    "}\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. Funkcje pomocnicze\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def make_preprocessor(df: pd.DataFrame):\n",
    "    num_cols = df.select_dtypes(include='number').columns\n",
    "    return ColumnTransformer([\n",
    "        ('scale', StandardScaler(), num_cols)\n",
    "    ])\n",
    "\n",
    "def total_cost(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return 10*fp + 500*fn\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4. Trenowanie + walidacja dla kaÅ¼dego wariantu\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "records = []\n",
    "\n",
    "for name, (X_train_v, y_train_v) in variants.items():\n",
    "    print(f\"â–¶ Strategia: {name}\")\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        ('prep', make_preprocessor(X_train_v)),\n",
    "        ('model', HistGradientBoostingClassifier(random_state=42))\n",
    "    ])\n",
    "    pipe.fit(X_train_v, y_train_v)\n",
    "\n",
    "    X_val_aligned = X_val.reindex(columns=X_train_v.columns, fill_value=0)\n",
    "    probs = pipe.predict_proba(X_val_aligned)[:, 1]\n",
    "\n",
    "    best_thr, best_cost = 0.5, np.inf\n",
    "    for t in np.linspace(0, 1, 201):\n",
    "        preds = (probs >= t).astype(int)\n",
    "        cost = total_cost(y_val, preds)\n",
    "        if cost < best_cost:\n",
    "            best_cost, best_thr = cost, t\n",
    "\n",
    "    records.append((name, best_cost, best_thr))\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5. Tabela wynikÃ³w\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "results = (pd.DataFrame(records, columns=['Metoda', 'Koszt_Calkowity', 'Najlepszy_Prog'])\n",
    "             .sort_values('Koszt_Calkowity')\n",
    "             .reset_index(drop=True))\n",
    "\n",
    "print(\"\\nðŸ“Š PorÃ³wnanie strategii obsÅ‚ugi obserwacji odstajÄ…cych:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b345927f",
   "metadata": {},
   "source": [
    "### ðŸ“Œ Conclusion\n",
    "\n",
    "Among the tested strategies, incorporating the **raw outlier score** (`with_score`) from Isolation Forest yielded the **lowest total cost (6520)**, outperforming both the baseline and other outlier-handling approaches. Interestingly, simply removing outliers (`without_outliers`) led to the worst performance, suggesting that preserving all observations while enriching the feature space with anomaly-related signals is more effective than exclusion. This highlights the value of leveraging unsupervised anomaly scores as informative features rather than relying solely on binary flags or data filtering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb47c60d",
   "metadata": {},
   "source": [
    "### ðŸ” Model comparison with `outlier_score` as a feature\n",
    "\n",
    "In this experiment, I evaluate the impact of using the **Isolation Forest outlier score** as an additional numerical feature. The pipeline includes standard preprocessing, a custom cost-sensitive scoring function, and stratified cross-validation.And the top two models are selected based on validation cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c76c9bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg | VAL cost = 8840 @ thr=0.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\igork\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf    | VAL cost = 6170 @ thr=0.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\igork\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "et    | VAL cost = 6370 @ thr=0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\igork\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gb    | VAL cost = 7040 @ thr=0.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\igork\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hgb   | VAL cost = 6370 @ thr=0.19\n",
      "xgb   | VAL cost = 8120 @ thr=0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\igork\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "3 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\igork\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\igork\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\igork\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 662, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\igork\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\catboost\\core.py\", line 5245, in fit\n",
      "    self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline, use_best_model,\n",
      "  File \"c:\\Users\\igork\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\catboost\\core.py\", line 2410, in _fit\n",
      "    self._train(\n",
      "  File \"c:\\Users\\igork\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\catboost\\core.py\", line 1790, in _train\n",
      "    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "  File \"_catboost.pyx\", line 5017, in _catboost._CatBoost._train\n",
      "  File \"_catboost.pyx\", line 5066, in _catboost._CatBoost._train\n",
      "_catboost.CatBoostError: catboost/libs/train_lib/dir_helper.cpp:26: Can't create train tmp dir: tmp\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\igork\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\igork\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\igork\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 662, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\igork\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\catboost\\core.py\", line 5245, in fit\n",
      "    self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline, use_best_model,\n",
      "  File \"c:\\Users\\igork\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\catboost\\core.py\", line 2410, in _fit\n",
      "    self._train(\n",
      "  File \"c:\\Users\\igork\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\catboost\\core.py\", line 1790, in _train\n",
      "    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "  File \"_catboost.pyx\", line 5017, in _catboost._CatBoost._train\n",
      "  File \"_catboost.pyx\", line 5066, in _catboost._CatBoost._train\n",
      "_catboost.CatBoostError: catboost/libs/train_lib/dir_helper.cpp:20: Can't create train working dir: catboost_info\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\igork\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [    nan -29588. -15160.  -9392. -10788. -10210. -15978. -16964. -13296.\n",
      " -10090.]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost | VAL cost = 7610 @ thr=0.97\n",
      "\n",
      "Top-2 wedÅ‚ug walidacji:\n",
      "   rf : Koszt=6170, prog=0.06\n",
      "   et : Koszt=6370, prog=0.15\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix, make_scorer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 0. Dane wejÅ›ciowe + outlier_score przez IsolationForest\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "iso = IsolationForest(contamination=0.01, random_state=42)\n",
    "iso.fit(X_train_imp)\n",
    "X_train_imp['outlier_score'] = -iso.decision_function(X_train_imp)\n",
    "X_test_imp['outlier_score']  = -iso.decision_function(X_test_imp)\n",
    "\n",
    "X_tr = X_train_imp\n",
    "X_val = X_test_imp\n",
    "y_tr = y_train\n",
    "y_val = y_test\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. WÅ‚asna metryka + sample_weight\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def total_cost(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return -(10 * fp + 500 * fn)\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "cost_scorer = make_scorer(total_cost, greater_is_better=True)\n",
    "\n",
    "w_tr  = np.where(y_tr == 1, 500, 10)\n",
    "w_val = np.where(y_val == 1, 500, 10)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. Preprocessing: uwzglÄ™dnia outlier_score\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "num_cols = X_tr.columns.drop(['is_outlier'], errors='ignore')\n",
    "flag_cols = ['is_outlier'] if 'is_outlier' in X_tr.columns else []\n",
    "\n",
    "prep = ColumnTransformer([\n",
    "    ('scale', StandardScaler(), num_cols),\n",
    "    ('pass', 'passthrough', flag_cols)\n",
    "])\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. Modele + RandomizedSearchCV\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from sklearn.linear_model  import LogisticRegression\n",
    "from sklearn.ensemble      import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble      import HistGradientBoostingClassifier, ExtraTreesClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "search_space = {\n",
    "    'logreg': (\n",
    "        LogisticRegression(max_iter=2000),\n",
    "        {'model__C': np.logspace(-3, 2, 12),\n",
    "         'model__solver': ['lbfgs', 'saga']}\n",
    "    ),\n",
    "    'rf': (\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        {'model__n_estimators': [400, 600],\n",
    "         'model__max_depth':   [None, 20]}\n",
    "    ),\n",
    "    'et': (\n",
    "        ExtraTreesClassifier(random_state=42),\n",
    "        {'model__n_estimators': [400, 600],\n",
    "         'model__max_depth':   [None, 20]}\n",
    "    ),\n",
    "    'gb': (\n",
    "        GradientBoostingClassifier(random_state=42),\n",
    "        {'model__n_estimators': [300, 500],\n",
    "         'model__learning_rate': [0.03, 0.07],\n",
    "         'model__max_depth': [2, 3]}\n",
    "    ),\n",
    "    'hgb': (\n",
    "        HistGradientBoostingClassifier(random_state=42),\n",
    "        {'model__max_iter': [300, 500],\n",
    "         'model__learning_rate': [0.03, 0.07],\n",
    "         'model__max_depth': [None, 6]}\n",
    "    ),\n",
    "    'xgb': (\n",
    "        xgb.XGBClassifier(\n",
    "            objective='binary:logistic',\n",
    "            eval_metric='logloss',\n",
    "            random_state=42,\n",
    "            tree_method='hist'),\n",
    "        {'model__n_estimators': [400, 600],\n",
    "         'model__max_depth': [3, 5],\n",
    "         'model__learning_rate': [0.05],\n",
    "         'model__subsample': [0.8, 1.0],\n",
    "         'model__colsample_bytree': [0.8, 1.0],\n",
    "         'model__scale_pos_weight': [50, 59]}\n",
    "    ),\n",
    "    'CatBoost': (\n",
    "        CatBoostClassifier(verbose=0, random_state=42),\n",
    "        {'model__iterations': [300, 500],\n",
    "         'model__depth': [3, 5],\n",
    "         'model__learning_rate': [0.03, 0.07],\n",
    "         'model__scale_pos_weight': [50, 59]}\n",
    "    )\n",
    "}\n",
    "\n",
    "best_on_val = []\n",
    "\n",
    "for name, (est, grid) in search_space.items():\n",
    "    pipe = Pipeline([('prep', prep), ('model', est)])\n",
    "    rs = RandomizedSearchCV(\n",
    "        pipe, grid, n_iter=10,\n",
    "        scoring=cost_scorer, cv=cv,\n",
    "        n_jobs=-1, random_state=42, verbose=0\n",
    "    )\n",
    "    rs.fit(X_tr, y_tr, model__sample_weight=w_tr)\n",
    "    best = rs.best_estimator_\n",
    "\n",
    "    probs = best.predict_proba(X_val)[:, 1]\n",
    "    ts = np.linspace(0, 1, 101)\n",
    "    best_thr, best_cost = 0.5, np.inf\n",
    "    for t in ts:\n",
    "        preds = (probs >= t).astype(int)\n",
    "        cost = 10*((preds==1)&(y_val==0)).sum() + 500*((preds==0)&(y_val==1)).sum()\n",
    "        if cost < best_cost:\n",
    "            best_cost, best_thr = cost, t\n",
    "    best_on_val.append((name, best, best_thr, best_cost))\n",
    "    print(f\"{name:5s} | VAL cost = {best_cost:.0f} @ thr={best_thr:.2f}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4. Wyniki: TOP-2 wedÅ‚ug walidacji\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "best_on_val.sort(key=lambda x: x[3])\n",
    "print(\"\\nTop-2 wedÅ‚ug walidacji:\")\n",
    "for name, _, thr, cost in best_on_val[:2]:\n",
    "    print(f\"{name:>5s} : Koszt={cost:.0f}, prog={thr:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8de791",
   "metadata": {},
   "source": [
    "### ðŸ“Œ Conclusion\n",
    "\n",
    "In this experiment, adding the raw `outlier_score` from Isolation Forest as a numerical feature led to solid improvements in model performance. The best results were achieved by tree-based ensembles: **Random Forest** yielded the lowest total cost (6170 at threshold 0.06), closely followed by **Extra Trees** (6370 at threshold 0.15).  \n",
    "\n",
    "Other models, including gradient boosting variants and CatBoost, performed less competitively under the defined cost function. Notably, CatBoost encountered temporary directory issues during training, which affected its search stability.\n",
    "\n",
    "These results confirm that enriching the feature space with unsupervised anomaly scores can enhance predictive performanceâ€”particularly for cost-sensitive tasksâ€”when used with robust ensemble models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09a053d",
   "metadata": {},
   "source": [
    "### ðŸ” PyTorch MLP with outlier score\n",
    "\n",
    "In this experiment, I trained a custom **MLP neural network** using PyTorch, incorporating the `outlier_score` from Isolation Forest as an input feature. The model was trained with class-based sample weights to reflect cost sensitivity, and early stopping was used for regularization. After training, the optimal threshold was selected based on the validation cost. This setup allowed testing how a neural network performs compared to tree-based models in the same cost-sensitive setting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc150fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 3621.4140 | Val Loss: 0.1747\n",
      "Epoch 02 | Train Loss: 2367.7945 | Val Loss: 0.1854\n",
      "Epoch 03 | Train Loss: 2121.5261 | Val Loss: 0.1244\n",
      "Epoch 04 | Train Loss: 2035.9989 | Val Loss: 0.1103\n",
      "Epoch 05 | Train Loss: 1860.0822 | Val Loss: 0.1248\n",
      "Epoch 06 | Train Loss: 1854.6625 | Val Loss: 0.0913\n",
      "Epoch 07 | Train Loss: 1815.1846 | Val Loss: 0.0980\n",
      "Epoch 08 | Train Loss: 1769.4707 | Val Loss: 0.0901\n",
      "Epoch 09 | Train Loss: 1597.6893 | Val Loss: 0.0796\n",
      "Epoch 10 | Train Loss: 1706.3344 | Val Loss: 0.0893\n",
      "Epoch 11 | Train Loss: 1609.0327 | Val Loss: 0.0836\n",
      "Epoch 12 | Train Loss: 1504.7230 | Val Loss: 0.0862\n",
      "Epoch 13 | Train Loss: 1489.2939 | Val Loss: 0.0986\n",
      "Epoch 14 | Train Loss: 1457.3969 | Val Loss: 0.0959\n",
      "Epoch 15 | Train Loss: 1533.6353 | Val Loss: 0.0874\n",
      "Epoch 16 | Train Loss: 1488.1416 | Val Loss: 0.0723\n",
      "Epoch 17 | Train Loss: 1422.8517 | Val Loss: 0.1120\n",
      "Epoch 18 | Train Loss: 1410.9046 | Val Loss: 0.0893\n",
      "Epoch 19 | Train Loss: 1362.0407 | Val Loss: 0.0783\n",
      "Epoch 20 | Train Loss: 1336.2748 | Val Loss: 0.0692\n",
      "Epoch 21 | Train Loss: 1320.3378 | Val Loss: 0.0670\n",
      "Epoch 22 | Train Loss: 1255.8712 | Val Loss: 0.1122\n",
      "Epoch 23 | Train Loss: 1305.7400 | Val Loss: 0.0796\n",
      "Epoch 24 | Train Loss: 1209.8171 | Val Loss: 0.0813\n",
      "Epoch 25 | Train Loss: 1202.3761 | Val Loss: 0.0723\n",
      "Epoch 26 | Train Loss: 1187.8222 | Val Loss: 0.0887\n",
      "Epoch 27 | Train Loss: 1169.6227 | Val Loss: 0.0737\n",
      "Epoch 28 | Train Loss: 1265.9932 | Val Loss: 0.0940\n",
      "Epoch 29 | Train Loss: 1231.2227 | Val Loss: 0.0770\n",
      "Epoch 30 | Train Loss: 1170.2967 | Val Loss: 0.0642\n",
      "Epoch 31 | Train Loss: 1251.3850 | Val Loss: 0.1054\n",
      "Epoch 32 | Train Loss: 1115.2283 | Val Loss: 0.0756\n",
      "Epoch 33 | Train Loss: 1083.2716 | Val Loss: 0.0668\n",
      "Epoch 34 | Train Loss: 1049.6841 | Val Loss: 0.0779\n",
      "Epoch 35 | Train Loss: 1149.9233 | Val Loss: 0.0626\n",
      "Epoch 36 | Train Loss: 1029.5881 | Val Loss: 0.0737\n",
      "Epoch 37 | Train Loss: 1095.4867 | Val Loss: 0.0845\n",
      "Epoch 38 | Train Loss: 1076.5510 | Val Loss: 0.0765\n",
      "Epoch 39 | Train Loss: 937.6021 | Val Loss: 0.0716\n",
      "Epoch 40 | Train Loss: 1051.5414 | Val Loss: 0.0558\n",
      "Epoch 41 | Train Loss: 924.1965 | Val Loss: 0.0753\n",
      "Epoch 42 | Train Loss: 1129.2511 | Val Loss: 0.0724\n",
      "Epoch 43 | Train Loss: 975.2301 | Val Loss: 0.0774\n",
      "Epoch 44 | Train Loss: 1002.0165 | Val Loss: 0.0713\n",
      "Epoch 45 | Train Loss: 996.5013 | Val Loss: 0.0526\n",
      "Epoch 46 | Train Loss: 904.3145 | Val Loss: 0.0915\n",
      "Epoch 47 | Train Loss: 1056.0939 | Val Loss: 0.0610\n",
      "Epoch 48 | Train Loss: 1004.7373 | Val Loss: 0.0581\n",
      "Epoch 49 | Train Loss: 992.7516 | Val Loss: 0.0753\n",
      "Epoch 50 | Train Loss: 1026.7904 | Val Loss: 0.0640\n",
      "Epoch 51 | Train Loss: 1006.6601 | Val Loss: 0.0809\n",
      "Epoch 52 | Train Loss: 922.1048 | Val Loss: 0.0667\n",
      "Epoch 53 | Train Loss: 1082.2135 | Val Loss: 0.0663\n",
      "Epoch 54 | Train Loss: 905.2240 | Val Loss: 0.0746\n",
      "Epoch 55 | Train Loss: 957.5849 | Val Loss: 0.0769\n",
      "â¹ï¸ Early stopping.\n",
      "\n",
      "ðŸ“‰ PyTorch MLP (with_score): VAL cost = 7750  @  thr = 0.16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ðŸ“Œ Data + add outlier_score\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "X_tr = X_train_imp.copy()\n",
    "X_val = X_test_imp.copy()\n",
    "y_tr = y_train.copy()\n",
    "y_val = y_test.copy()\n",
    "\n",
    "# IsolationForest: only on train\n",
    "iso = IsolationForest(contamination=0.01, random_state=42)\n",
    "iso.fit(X_tr)\n",
    "X_tr['outlier_score'] = -iso.decision_function(X_tr)\n",
    "X_val['outlier_score'] = -iso.decision_function(X_val)\n",
    "\n",
    "# Remove is_outlier and is_lof, keep only outlier_score\n",
    "cols_to_use = [c for c in X_tr.columns if c not in ['is_outlier', 'is_lof']]\n",
    "X_tr = X_tr[cols_to_use]\n",
    "X_val = X_val[cols_to_use]\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ðŸ“Œ Scaling\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "scaler = StandardScaler()\n",
    "X_tr_scaled = scaler.fit_transform(X_tr)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ðŸ“Œ Class weights\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "weights_np = np.where(y_tr == 1, 500, 10)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ðŸ“Œ Dataset and DataLoader\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class WeightedDataset(Dataset):\n",
    "    def __init__(self, X, y, sample_weights):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.values, dtype=torch.float32)\n",
    "        self.w = torch.tensor(sample_weights, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.w[idx]\n",
    "\n",
    "train_ds = WeightedDataset(X_tr_scaled, y_tr, weights_np)\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ðŸ“Œ Model\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze()\n",
    "\n",
    "model = MLP(input_dim=X_tr.shape[1])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ðŸ“Œ Training\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss(reduction='none')\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience, patience_counter = 10, 0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb, wb in train_dl:\n",
    "        xb, yb, wb = xb.to(device), yb.to(device), wb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        weighted_loss = (loss * wb).mean()\n",
    "        weighted_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += weighted_loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_input = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
    "        val_target = torch.tensor(y_val.values, dtype=torch.float32).to(device)\n",
    "        val_preds = model(val_input)\n",
    "        val_loss = nn.BCELoss()(val_preds, val_target).item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d} | Train Loss: {total_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_state = model.state_dict()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"â¹ï¸ Early stopping.\")\n",
    "            break\n",
    "\n",
    "model.load_state_dict(best_state)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ðŸ“Œ Predictions and threshold selection\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    probs = model(torch.tensor(X_val_scaled, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "\n",
    "best_thr, best_cost = 0.5, float(\"inf\")\n",
    "for t in np.linspace(0, 1, 101):\n",
    "    preds = (probs >= t).astype(int)\n",
    "    cm = confusion_matrix(y_val, preds, labels=[0, 1])\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.shape == (2, 2) else (0, 0, 0, 0)\n",
    "    cost = 10 * fp + 500 * fn\n",
    "    if cost < best_cost:\n",
    "        best_cost = cost\n",
    "        best_thr = t\n",
    "\n",
    "print(f\"\\nðŸ“‰ PyTorch MLP (with_score): VAL cost = {best_cost:.0f}  @  thr = {best_thr:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794102b8",
   "metadata": {},
   "source": [
    "### ðŸ“Œ Conclusion\n",
    "\n",
    "The PyTorch MLP reached a validation cost of **7750** at an optimal threshold of **0.16**. While the model demonstrated solid learning progress with consistent validation improvement and effective early stopping, it still slightly underperformed compared to the best tree-based models. This suggests that while neural networks can handle cost-sensitive tasks well, ensemble methods remain more effective for this specific structured, tabular dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f46328",
   "metadata": {},
   "source": [
    "### ðŸ” Final Evaluation: Blending models with outlier-based enrichment\n",
    "\n",
    "In the final stage of this project, I combined the strongest individual models â€” **Random Forest** (trained on raw features with `outlier_score`) and **Extra Trees** (trained with an `is_outlier` flag) â€” to build an ensemble through **probability blending**.\n",
    "\n",
    "Key elements of this step:\n",
    "\n",
    "- **Different representations of outliers** were used to train two separate pipelines: one leveraging numerical outlier scores, and the other using a binary outlier flag.\n",
    "- **Both models were trained with sample weights** to penalize false negatives more heavily, as defined by the domain-specific cost function `10 Ã— FP + 500 Ã— FN`.\n",
    "\n",
    "#### ðŸ§ª Blending Strategy\n",
    "\n",
    "Two linear blends of predicted probabilities were evaluated:\n",
    "- `Blend 0.5 / 0.5` â€” equal weight to RF and ET predictions.\n",
    "- `Blend 0.9 / 0.1` â€” prioritizing Random Forest, which previously had the lowest individual cost.\n",
    "\n",
    "Before final testing, I experimented with various blending weights and thresholds on the **validation dataset** to find the most effective combination. The `0.9 / 0.1` blend consistently achieved **better results**, outperforming the `0.5 / 0.5` blend by approximately **200 units of cost**, though exact figures may vary slightly.\n",
    "\n",
    "Despite the weaker validation performance, I chose to include the `0.5 / 0.5` blend in the final evaluation. The rationale was to **avoid over-reliance on a single model** and to test a more balanced scenario where both models contribute equally to the final decision. This helped assess whether diversity in model input (i.e., different representations of outliers) might offer robustness, even at the expense of a slightly higher cost.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83cf248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Skipping PyTorch model:  No module named 'trained_pytorch_model'\n",
      "Model / Strategy                         | Score | Type 1 Faults (FP) | Type 2 Faults (FN)\n",
      "--------------------------------------------------------------------------------\n",
      "Blend 0.5 / 0.5 @ thr = 0.080            | 11240 |                 274 |                  17\n",
      "Blend 0.9 / 0.1 @ thr = 0.085            | 12530 |                 253 |                  20\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, IsolationForest\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. Load train and test datasets\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "train_df = pd.read_csv(\"aps_failure_training_set.csv\", skiprows=20, na_values='na')\n",
    "test_df  = pd.read_csv(\"aps_failure_test_set.csv\", skiprows=20, na_values='na')\n",
    "\n",
    "y_train = train_df.pop(\"class\").map({'pos': 1, 'neg': 0})\n",
    "y_test  = test_df.pop(\"class\").map({'pos': 1, 'neg': 0})\n",
    "X_train = train_df\n",
    "X_test  = test_df\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. Imputation (median strategy)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train_imp = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_imp  = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. Outlier detection (Isolation Forest)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "iso = IsolationForest(contamination=0.01, random_state=42)\n",
    "iso.fit(X_train_imp)\n",
    "\n",
    "# RandomForest: using outlier score\n",
    "X_train_rf = X_train_imp.copy()\n",
    "X_test_rf  = X_test_imp.copy()\n",
    "X_train_rf['outlier_score'] = -iso.decision_function(X_train_rf)\n",
    "X_test_rf['outlier_score']  = -iso.decision_function(X_test_rf)\n",
    "\n",
    "# ExtraTrees: using outlier flag\n",
    "X_train_et = X_train_imp.copy()\n",
    "X_test_et  = X_test_imp.copy()\n",
    "X_train_et['is_outlier'] = (iso.predict(X_train_et) == -1).astype(int)\n",
    "X_test_et['is_outlier']  = (iso.predict(X_test_et) == -1).astype(int)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4. Class weights (to balance classes)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "w_train = np.where(y_train == 1, 500, 10)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5. Train RandomForest model\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "prep_rf = ColumnTransformer([\n",
    "    ('scale', StandardScaler(), X_train_rf.columns)\n",
    "])\n",
    "pipe_rf = Pipeline([\n",
    "    ('prep', prep_rf),\n",
    "    ('model', RandomForestClassifier(n_estimators=600, random_state=42))\n",
    "])\n",
    "pipe_rf.fit(X_train_rf, y_train, model__sample_weight=w_train)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 6. Train ExtraTrees model\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "prep_et = ColumnTransformer([\n",
    "    ('scale', StandardScaler(), X_train_et.columns.drop('is_outlier')),\n",
    "    ('pass', 'passthrough', ['is_outlier'])\n",
    "])\n",
    "pipe_et = Pipeline([\n",
    "    ('prep', prep_et),\n",
    "    ('model', ExtraTreesClassifier(n_estimators=600, random_state=42))\n",
    "])\n",
    "pipe_et.fit(X_train_et, y_train, model__sample_weight=w_train)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 7. Final predictions on test set\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "probs_rf = pipe_rf.predict_proba(X_test_rf)[:, 1]\n",
    "probs_et = pipe_et.predict_proba(X_test_et)[:, 1]\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 8. Add blended model evaluations\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "blends = [\n",
    "    (\"Blend 0.5 / 0.5 @ thr = 0.080\", 0.5, 0.080),\n",
    "    (\"Blend 0.9 / 0.1 @ thr = 0.085\", 0.9, 0.085)\n",
    "]\n",
    "\n",
    "for name, alpha, threshold in blends:\n",
    "    blended_probs = alpha * probs_rf + (1 - alpha) * probs_et\n",
    "    preds = (blended_probs >= threshold).astype(int)\n",
    "    fp = ((preds == 1) & (y_test == 0)).sum()\n",
    "    fn = ((preds == 0) & (y_test == 1)).sum()\n",
    "    score = 10 * fp + 500 * fn\n",
    "    results.append((name, score, fp, fn))\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 9. Display results\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "results.sort(key=lambda x: x[1])\n",
    "\n",
    "print(\"Model / Strategy\".ljust(40), \"| Score | Type 1 Faults (FP) | Type 2 Faults (FN)\")\n",
    "print(\"-\" * 80)\n",
    "for name, score, fp, fn in results:\n",
    "    print(f\"{name.ljust(40)} | {score:5d} | {fp:19d} | {fn:19d}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fba3b26",
   "metadata": {},
   "source": [
    "### ðŸ“‰ Final Results vs. IDA 2016 Benchmarks\n",
    "\n",
    "| Model / Strategy              | Score | Type 1 Faults (FP) | Type 2 Faults (FN) |\n",
    "|------------------------------|-------|---------------------|---------------------|\n",
    "| **Blend 0.5 / 0.5 @ thr=0.080** | **11240** | 274                 | 17                  |\n",
    "| Blend 0.9 / 0.1 @ thr=0.085    | 12530 | 253                 | 20                  |\n",
    "\n",
    "#### ðŸ†š Reference: Top 3 from IDA 2016 Challenge\n",
    "\n",
    "| Team                                             | Score | Type 1 Faults (FP) | Type 2 Faults (FN) |\n",
    "|--------------------------------------------------|--------|---------------------|---------------------|\n",
    "| Camila F. Costa & Mario A. Nascimento            | 9920   | 542                 | 9                   |\n",
    "| Christopher Gondek et al.                        | 10900  | 490                 | 12                  |\n",
    "| Sumeet Garnaik et al.                            | 11480  | 398                 | 15                  |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Final Conclusion\n",
    "\n",
    "The blended models achieved scores in the **same range as the top performers** from the 2016 IDA Industrial Challenge, despite using a fully self-constructed pipeline without access to their internal techniques. The best-performing blend (`0.5 / 0.5 @ thr=0.080`) reached a **score of 11240**, just **320 points** behind the second-place solution from the original competition.\n",
    "\n",
    "Interestingly, this blend had **fewer false positives** but **slightly more false negatives** than some of the historical entries â€” a direct trade-off shaped by the custom cost function prioritizing FN heavily.\n",
    "\n",
    "This result validates the effectiveness of:\n",
    "- Incorporating **unsupervised outlier features** (`outlier_score`, `is_outlier`);\n",
    "- Using **ensemble methods with cost-weighted training**;\n",
    "- Applying **simple blending strategies** to capture complementary model behavior.\n",
    "\n",
    "The outcome demonstrates that **a well-engineered, interpretable approach** can be highly competitive with past state-of-the-art results, and serves as a strong baseline for further improvements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ee348a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Models and blend configurations saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 11. Save models and metadata\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "joblib.dump(pipe_rf, \"model_random_forest.pkl\")\n",
    "joblib.dump(pipe_et, \"model_extra_trees.pkl\")\n",
    "\n",
    "blend_info = {\n",
    "    \"blend_0.5_0.5\": {\"alpha\": 0.5, \"threshold\": 0.080},\n",
    "    \"blend_0.9_0.1\": {\"alpha\": 0.9, \"threshold\": 0.085}\n",
    "}\n",
    "joblib.dump(blend_info, \"blend_config.pkl\")\n",
    "\n",
    "print(\"\\nâœ… Models and blend configurations saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
